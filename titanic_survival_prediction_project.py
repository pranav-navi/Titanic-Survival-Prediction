# -*- coding: utf-8 -*-
"""Titanic Survival Prediction Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uv-3X4n1XWNGhi1y3XYTd8GPfPWDs1LN

# Titanic Survival Prediction using Logistic Regression

The main goal of this project is to predict whether a passenger survived or not in the Titanic shipwreck using passenger details like age, gender, class, and ticket fare.


This is a Supervised Machine Learning Classification problem — specifically Binary Classification, because there are two possible outcomes:


1 = Survived


0 = Did Not Survive


We train the computer to learn patterns from existing passenger data so it can predict new cases accurately.

Step 1: Loading the Dataset
We imported the Titanic dataset using Seaborn or
Kaggle’s train.csv.
We use this step:
To get our working data into Python for analysis and modeling.
"""

!pip install -q seaborn scikit-learn

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# OPTION A (easy): seaborn built-in Titanic (quick)
try:
    df = sns.load_dataset('titanic')
    print("Loaded seaborn 'titanic' dataset.")
except Exception as e:
    df = None
    print("Seaborn load failed:", e)

# OPTION B (preferred if you have Kaggle CSV):
# from google.colab import files
# uploaded = files.upload()   # use this in Colab UI to upload train.csv
# df = pd.read_csv('train.csv')  # change filename if different
# print("Loaded Kaggle train.csv")

# Basic checks (paste outputs here if you want help)
print("\nShape:", df.shape)
print("\nColumns:", df.columns.tolist())
print("\nMissing values per column:\n", df.isna().sum())
display(df.head(8))

"""Step 2 Data Cleaning and Feature Selection

Standardize column names to ensure consistency between different dataset versions.

Select key input features and the target variable (Survived).

Check for missing values, convert data types, and inspect basic feature details.
"""

if 'class' in df.columns and 'Pclass' not in df.columns:
    df = df.rename(columns={'class': 'Pclass'})
if 'sex' in df.columns and 'Sex' not in df.columns:
    df = df.rename(columns={'sex': 'Sex'})
if 'survived' in df.columns and 'Survived' not in df.columns:
    df = df.rename(columns={'survived': 'Survived'})
if 'fare' in df.columns and 'Fare' not in df.columns:
    df = df.rename(columns={'fare': 'Fare'})
if 'sibsp' in df.columns and 'SibSp' not in df.columns:
    df = df.rename(columns={'sibsp': 'SibSp'})
if 'parch' in df.columns and 'Parch' not in df.columns:
    df = df.rename(columns={'parch': 'Parch'})
if 'embarked' in df.columns and 'Embarked' not in df.columns:
    df = df.rename(columns={'embarked': 'Embarked'})
if 'age' in df.columns and 'Age' not in df.columns:
    df = df.rename(columns={'age': 'Age'})

# Ensure target exists
if 'Survived' not in df.columns:
    raise ValueError("Target column 'Survived' not found. If you have Kaggle train.csv, load it and rerun Cell 1.")

# Features we will use (exact set you requested)
features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']

missing = [c for c in features if c not in df.columns]
if missing:
    raise ValueError(f"Missing expected columns: {missing}. Adjust dataset or column names.")

# Create X and y
X = df[features].copy()
y = df['Survived'].astype(int).copy()

# Force numeric where expected
for col in ['Pclass','Age','SibSp','Parch','Fare']:
    if col in X.columns:
        X[col] = pd.to_numeric(X[col], errors='coerce')

# Quick prints to confirm
print("Selected features shape:", X.shape)
print("\nMissing values in selected features:\n", X.isna().sum())
print("\nTarget distribution:")
print(y.value_counts(normalize=True))
print("\nUnique values - Sex:", X['Sex'].unique())
print("Unique values - Embarked:", X['Embarked'].unique())
display(X.head(8))

"""Step3 Data Preprocessing and Logistic Regression Model

Apply preprocessing using pipelines for numeric (scaling) and categorical (encoding) features.

Split the dataset into training and testing sets, then train a Logistic Regression model.

Evaluate performance using accuracy, ROC AUC, and a confusion matrix visualization.
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve
import seaborn as sns
import matplotlib.pyplot as plt


numeric_features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']
categorical_features = ['Sex', 'Embarked']

numeric_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer([
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
])


X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Build pipeline with logistic regression
pipe = Pipeline([
    ('preprocessor', preprocessor),
    ('clf', LogisticRegression(max_iter=1000))
])

# Train
pipe.fit(X_train, y_train)

# Predict & evaluate
y_pred = pipe.predict(X_test)
y_proba = pipe.predict_proba(X_test)[:,1]

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("ROC AUC:", roc_auc_score(y_test, y_proba))

# Confusion matrix plot
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Logistic Regression)')
plt.show()

"""Step 4 ROC Curve, Predictions, and Model Saving

Plot the ROC curve to visualize model performance.

Save test predictions with actual, predicted, and probability values in a CSV file.

Export the trained pipeline model using joblib for future use.
"""

from sklearn.metrics import roc_curve
import joblib

# ROC curve
fpr, tpr, _ = roc_curve(y_test, y_proba)
plt.figure(figsize=(6,4))
plt.plot(fpr, tpr, label=f'ROC AUC = {roc_auc_score(y_test,y_proba):.3f}')
plt.plot([0,1],[0,1],'--', color='grey')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve (Logistic Regression)')
plt.legend()
plt.show()

# Save test predictions dataframe
pred_df = X_test.copy()
pred_df['Actual'] = y_test.values
pred_df['Predicted'] = y_pred
pred_df['Prob_Survived'] = y_proba
pred_df.to_csv('titanic_test_predictions.csv', index=False)
print("Saved titanic_test_predictions.csv")

# Save trained pipeline
joblib.dump(pipe, 'titanic_logistic_pipeline.joblib')
print("Saved trained pipeline to 'titanic_logistic_pipeline.joblib'")

"""Step 5 Feature Importance and Interpretation

Extract feature names and corresponding logistic regression coefficients.

Create a table showing each feature’s influence on survival probability.

Visualize coefficients using a bar plot to interpret positive and negative impacts.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Helper: get feature names after preprocessing
preproc = pipe.named_steps['preprocessor']
# Numeric names (in order)
num_feats = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']
num_feats = [f for f in num_feats if f in X.columns]  # keep only present ones

# Get categorical one-hot names (if any)
cat_feats = ['Sex', 'Embarked']
cat_feats = [f for f in cat_feats if f in X.columns]

# Fit preprocessor on full X to ensure transformers are ready (safe)
preproc.fit(X)

# Build full feature name list
feature_names = []
if len(num_feats) > 0:
    feature_names.extend(num_feats)

if len(cat_feats) > 0:
    # Extract onehot encoder from the ColumnTransformer
    # Find the 'cat' transformer index
    try:
        cat_transformer = preproc.named_transformers_['cat']
        ohe = cat_transformer.named_steps['onehot']
        ohe_names = ohe.get_feature_names_out(cat_feats).tolist()
        feature_names.extend(ohe_names)
    except Exception as e:
        # fallback: create simple names to keep order
        for c in cat_feats:
            feature_names.append(f"{c}_*")
        print("Warning extracting OHE names:", e)

# Get logistic coefficients
clf = pipe.named_steps['clf']
coefs = clf.coef_[0]    # shape (n_features,)

# Safety: if lengths mismatch, display sizes
print("Len(feature_names):", len(feature_names), "Len(coefs):", len(coefs))

# Create DataFrame mapping feature->coef (truncate names if necessary)
feat_df = pd.DataFrame({
    'feature': feature_names[:len(coefs)],
    'coefficient': coefs[:len(feature_names)]
})

# Sort by absolute effect
feat_df['abs_coef'] = feat_df['coefficient'].abs()
feat_df = feat_df.sort_values('abs_coef', ascending=False).reset_index(drop=True)
feat_df = feat_df.drop(columns='abs_coef')

# Show top features
print("\nTop features by coefficient magnitude:")
display(feat_df.head(20))

# Plot coefficients (direction matters: positive -> higher chance of survival)
plt.figure(figsize=(8,6))
sns.barplot(x='coefficient', y='feature', data=feat_df, palette='coolwarm')
plt.title('Logistic Regression Coefficients (higher -> increases log-odds of survival)')
plt.axvline(0, color='k', linestyle='--', linewidth=0.8)
plt.tight_layout()
plt.show()

"""Step 6 Model Validation and Hyperparameter Tuning

Perform 5-fold cross-validation to assess baseline model performance.

Use GridSearchCV to tune the regularization parameter C for optimal accuracy and ROC AUC.

Evaluate and save the best-performing logistic regression model for future use.
"""

from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.metrics import roc_auc_score, accuracy_score, classification_report

# 1) Quick cross-validation (ROC AUC) for current pipeline
cv_scores = cross_val_score(pipe, X, y, cv=5, scoring='roc_auc')
print("Baseline Logistic 5-fold CV ROC AUC: %.4f (± %.4f)" % (cv_scores.mean(), cv_scores.std()))

# 2) GridSearch to tune C (regularization strength)
param_grid = {
    'clf__C': [0.01, 0.1, 1, 10, 100]
}
gs = GridSearchCV(pipe, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)
gs.fit(X_train, y_train)

print("\nBest params from GridSearch:", gs.best_params_)
print("Best CV ROC AUC:", gs.best_score_)

# 3) Evaluate best estimator on test set
best_pipe = gs.best_estimator_
y_pred_best = best_pipe.predict(X_test)
y_proba_best = best_pipe.predict_proba(X_test)[:,1]

print("\nTest set evaluation (best estimator):")
print("Accuracy:", accuracy_score(y_test, y_pred_best))
print("ROC AUC:", roc_auc_score(y_test, y_proba_best))
print("\nClassification Report:\n", classification_report(y_test, y_pred_best))

# 4) (Optional) Save best model file for submission/demo
import joblib
joblib.dump(best_pipe, 'titanic_logistic_best_pipeline.joblib')
print("Saved best pipeline as 'titanic_logistic_best_pipeline.joblib'")

"""Step 7 Plain-English Feature Interpretation

Extract feature names and coefficients from the trained logistic regression model.

Determine whether each feature increases or decreases the survival probability.

Generate readable, slide-ready explanations describing feature effects in plain English.
"""

import numpy as np
import pandas as pd

# Ensure pipe exists
if 'pipe' not in globals():
    raise RuntimeError("No trained pipeline `pipe` found. Ensure you ran the training cell and `pipe` is in the namespace.")

# Extract feature names after preprocessing
preproc = pipe.named_steps['preprocessor']
# numeric features list used earlier (if Title added, pipeline likely uses more categorical)
# We'll attempt to reconstruct names dynamically
num_feats = ['Pclass','Age','SibSp','Parch','Fare']
num_feats = [f for f in num_feats if f in X.columns]

cat_feats = [c for c in X.columns if c not in num_feats]
# Fit preprocessor to get OHE names (safe)
preproc.fit(X)
feature_names = []
if len(num_feats)>0:
    feature_names.extend(num_feats)
if len(cat_feats)>0:
    try:
        cat_tr = preproc.named_transformers_['cat']
        ohe = cat_tr.named_steps['onehot']
        ohe_names = ohe.get_feature_names_out(cat_feats).tolist()
        feature_names.extend(ohe_names)
    except Exception:
        # fallback, create generic cat names
        for c in cat_feats:
            feature_names.append(c + "_*")

# Extract coefficients
clf = pipe.named_steps['clf']
coefs = clf.coef_[0]
# Align lengths
min_len = min(len(feature_names), len(coefs))
feature_names = feature_names[:min_len]
coefs = coefs[:min_len]

coef_df = pd.DataFrame({'feature': feature_names, 'coef': coefs})
coef_df['effect'] = coef_df['coef'].apply(lambda x: 'increases' if x>0 else 'decreases')
coef_df['coef_round'] = coef_df['coef'].round(3)

# Produce English lines
lines = []
for _, row in coef_df.sort_values('coef', ascending=False).iterrows():
    feat = row['feature']
    sign = row['effect']
    val = row['coef_round']
    if '*' in feat:
        # generic categorical fallback
        lines.append(f"{feat}: {sign} likelihood of survival (coef={val}).")
    else:
        # nicer formatting for one-hot names like Sex_female or Title_Miss
        pretty = feat.replace('onehot__', '').replace('Sex_', 'Sex=').replace('Embarked_', 'Embarked=').replace('Title_', 'Title=')
        pretty = pretty.replace('_', ' ')
        lines.append(f"{pretty} {sign} probability of survival (coefficient = {val}).")

# Print out slide-ready bullets
print("Slide-ready feature explanation lines:\n")
for l in lines:
    print("- " + l)

"""Step 8 Advanced Grid Search for Logistic Regression

Perform an extensive GridSearchCV to tune multiple logistic regression hyperparameters.

Identify the best parameter combination that maximizes ROC AUC score.

Evaluate the optimized model’s performance on the test dataset.
"""

from sklearn.model_selection import GridSearchCV
pipe = Pipeline([('preprocessor', preprocessor), ('clf', LogisticRegression(max_iter=2000))])

param_grid = {
    'clf__penalty': ['l1','l2','elasticnet','none'],
    'clf__C': [0.01,0.1,1,10,100],
    'clf__solver': ['saga','liblinear','newton-cg','lbfgs'],
    # elasticnet needs l1_ratio; GridSearch will skip invalid combos automatically but might warn
    'clf__l1_ratio': [None, 0.1, 0.5, 0.9]
}

gs = GridSearchCV(pipe, param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1)
gs.fit(X_train, y_train)
print("Best params:", gs.best_params_)
best = gs.best_estimator_
y_pred = best.predict(X_test)
y_proba = best.predict_proba(X_test)[:,1]
from sklearn.metrics import classification_report,roc_auc_score
print("ROC AUC (test):", roc_auc_score(y_test, y_proba))
print(classification_report(y_test, y_pred))

"""Step 9 Export Final Predictions and Model

Use the final trained model to generate predictions and survival probabilities.

Save results in a CSV file for further analysis or submission.

Export the finalized model using joblib for future reuse.
"""

import joblib
final_model = best if 'best' in globals() else pipe
# Make preds
y_pred = final_model.predict(X_test)
y_proba = final_model.predict_proba(X_test)[:,1]

out = X_test.copy().reset_index(drop=True)
out['Actual'] = y_test.reset_index(drop=True)
out['Predicted'] = y_pred
out['Prob_Survived'] = y_proba
out.to_csv('titanic_test_results_export.csv', index=False)
print("Saved titanic_test_results_export.csv")

# Save model
joblib.dump(final_model, 'titanic_final_model.joblib')
print("Saved model as titanic_final_model.joblib")